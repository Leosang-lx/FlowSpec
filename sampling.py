import math
import torch.nn.functional as F
import torch


def top_k_masking(logits, top_k=100):
    """
    Use -inf to mask the values out of TOP_K
    :param logits: input logits (batch, vocab_size)
    :param top_k: top_k value
    :return: logits masked by -inf using TOP_K sampling
    """
    top_k_values, top_k_indices = torch.topk(logits, top_k)

    logits_top_k_masked = torch.full_like(logits, float('-inf'))
    logits_top_k_masked.scatter_(dim=-1, index=top_k_indices, src=logits)

    return logits_top_k_masked


def top_p_sampling(logits: torch.Tensor, top_p=0.9):
    """
    Use TOP_P sampling to generate token based on input logits
    :param logits: input logits
    :param top_p: top_p value
    :return: samples tokens using TOP_P sampling
    """
    # logits shape: (batch_size, vocab_size)
    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
    # mask logits based on their probabilities
    sorted_probs = F.softmax(sorted_logits, dim=-1)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    # index to remove in sorted order
    sorted_indices_to_remove = cumulative_probs > top_p
    # keep the element on the boundary, and keep at least one element
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    # mask sorted_logits with -inf
    sorted_logits[sorted_indices_to_remove] = -float('Inf')
    # cal probs based on the input indices
    filtered_sorted_probs = F.softmax(sorted_logits, dim=-1)
    # sampling
    sampled_sorted_indices = torch.multinomial(filtered_sorted_probs, num_samples=1)
    # transform the sampled ordered indices to the original indices
    sampled_tokens = sorted_indices.gather(dim=-1, index=sampled_sorted_indices)

    return sampled_tokens


def apply_sampling(logits: torch.Tensor, top_k=None, top_p=None, temperature=1.0):
    """
    Applies top-k/top-p sampling on the given logits
    :param logits: input logits tensor
    :param top_k: top_k value
    :param top_p: top_p value
    :param temperature: temperature value for scaling
    :return: tensor of sampled token index according to the vocabulary size
    """
    # scaling with temperature factor
    logits /= temperature
    if top_k is not None and top_k > 0:
        # Apply top-k sampling
        logits = top_k_masking(logits, top_k)
    if top_p is not None and top_p < 1.0:
        # Apply top-p sampling
        sampled_tokens = top_p_sampling(logits, top_p)
    else:
        probs = F.softmax(logits, dim=-1)
        sampled_tokens = torch.multinomial(probs, num_samples=1)

    return sampled_tokens


def test_sampling_masking():
    # test top_k masking
    logits = torch.randn((3, 10))

    # generated by GPT
    top_k = 5
    logits_copy = logits.detach().clone()
    masked_logits_top_k_only = apply_sampling(logits, top_k=top_k)
    # my code
    top_k_values, top_k_indices = torch.topk(logits_copy, top_k, dim=-1)
    top_k_mask = torch.zeros_like(logits_copy, dtype=torch.bool)
    top_k_mask.scatter_(dim=-1, index=top_k_indices, src=torch.ones_like(logits_copy, dtype=torch.bool))
    logits_copy[~top_k_mask] = -float('inf')
    # test for the result of top_k sampling
    print(torch.equal(masked_logits_top_k_only, logits_copy))

    # test top_p masking
    logits = torch.randn((3, 10))
    logits_copy = logits.detach().clone()
    print(logits)
    # the logits masked by top p sampling generated by GPT
    top_p = 0.8
    masked_logits_top_p_only = apply_sampling(logits, top_p=top_p)

    # implementation manually: 直接通过logits的大小值来获取
    # 获得前p部分的logits indices

    sorted_probs, sorted_indices = torch.sort(logits_copy, descending=True, dim=-1)
    cut_off_index = math.floor(logits_copy.shape[-1] * top_p)
    top_p_indices = sorted_indices[:, :cut_off_index]
    # 根据top p indices来获得对应的top p masking
    top_p_mask = torch.zeros_like(logits_copy, dtype=torch.bool)
    top_p_mask.scatter_(dim=-1, index=top_p_indices, src=torch.ones_like(logits_copy, dtype=torch.bool))
    logits_copy[~top_p_mask] = -float('inf')

    # test for the result of top_p sampling
    print(masked_logits_top_p_only)
    print(logits_copy)
    # print(torch.eq(masked_logits_top_p_only, logits_copy))


if __name__ == '__main__':
    torch.manual_seed(666)
    test_sampling_masking()
